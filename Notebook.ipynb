{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd8cd44-ee4c-4f9e-806e-6fc33ffcbb60",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4a082c-288f-45c5-8f0f-4abb8883a302",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc504b65-c505-415e-a7ec-cb6f085d4531",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "\n",
    "train_data = X_train.merge(y_train, on='id', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5785340e-32e7-4ff3-b40c-c164b390be0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:  27934\n",
      "name:  16395\n",
      "has_test:  2\n",
      "response_letter_required:  2\n",
      "salary_from:  479\n",
      "salary_currency:  1\n",
      "salary_gross:  3\n",
      "published_at:  27173\n",
      "created_at:  27173\n",
      "employer_name:  13318\n",
      "description:  23682\n",
      "area_id:  156\n",
      "area_name:  156\n",
      "\n",
      "id:  9312\n",
      "name:  6275\n",
      "has_test:  2\n",
      "response_letter_required:  2\n",
      "salary_from:  285\n",
      "salary_currency:  1\n",
      "salary_gross:  3\n",
      "published_at:  9195\n",
      "created_at:  9195\n",
      "employer_name:  6025\n",
      "description:  8394\n",
      "area_id:  118\n",
      "area_name:  118\n"
     ]
    }
   ],
   "source": [
    "for column_name in X_train.columns:\n",
    "    print(f'{column_name}: ', len(pd.unique(X_train[column_name])))\n",
    "    \n",
    "print()\n",
    "\n",
    "for column_name in X_test.columns:\n",
    "    print(f'{column_name}: ', len(pd.unique(X_test[column_name])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5328bb6-13c4-4364-93a2-88ca76532fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans in train\n",
      " id                             0\n",
      "name                           0\n",
      "has_test                       0\n",
      "response_letter_required       0\n",
      "salary_from                 4032\n",
      "salary_currency                0\n",
      "salary_gross                 148\n",
      "published_at                   0\n",
      "created_at                     0\n",
      "employer_name                  0\n",
      "description                    1\n",
      "area_id                        0\n",
      "area_name                      0\n",
      "salary_to                      0\n",
      "dtype: int64\n",
      "\n",
      "Nans in test\n",
      " id                             0\n",
      "name                           0\n",
      "has_test                       0\n",
      "response_letter_required       0\n",
      "salary_from                 1388\n",
      "salary_currency                0\n",
      "salary_gross                  49\n",
      "published_at                   0\n",
      "created_at                     0\n",
      "employer_name                  0\n",
      "description                    0\n",
      "area_id                        0\n",
      "area_name                      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "has_nan = train_data.isna().sum()\n",
    "print(\"Nans in train\\n\", has_nan)\n",
    "\n",
    "has_nan = X_test.isna().sum()\n",
    "print(\"\\nNans in test\\n\", has_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedd98a-ad63-41f6-a5b4-5f5845b04ce5",
   "metadata": {},
   "source": [
    "## Remove extra columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db3ecf29-b030-4fab-a610-e025f286d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.drop(columns=['salary_currency', 'published_at', 'created_at', 'area_name'])\n",
    "X_test = X_test.drop(columns=['salary_currency', 'published_at', 'created_at', 'area_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4203ad-fcd6-41fa-b03c-587584430820",
   "metadata": {},
   "source": [
    "## Remove rows with empty description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73b3cd96-1a9f-4763-a0b7-f8df00f11e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12194]\n"
     ]
    }
   ],
   "source": [
    "idxs = train_data[train_data['description'].isna()].index.tolist()\n",
    "print(idxs)\n",
    "\n",
    "train_data = train_data.drop(idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738bdd7-e99d-482e-a143-9aaf37e971d0",
   "metadata": {},
   "source": [
    "# Process text fields with NLP\n",
    "\n",
    "Extracting lemmas from text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f138418-831e-4c0a-9e22-fbc044f9cede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocess_name: 0\n",
      "preprocess_name: 3000\n",
      "preprocess_name: 6000\n",
      "preprocess_name: 9000\n",
      "preprocess_description: 0\n",
      "preprocess_description: 3000\n",
      "preprocess_description: 6000\n",
      "preprocess_description: 9000\n",
      "preprocess_employer_name: 0\n",
      "preprocess_employer_name: 3000\n",
      "preprocess_employer_name: 6000\n",
      "preprocess_employer_name: 9000\n",
      "preprocess_name: 0\n",
      "preprocess_name: 3000\n",
      "preprocess_name: 6000\n",
      "preprocess_name: 9000\n",
      "preprocess_name: 12000\n",
      "preprocess_name: 15000\n",
      "preprocess_name: 18000\n",
      "preprocess_name: 21000\n",
      "preprocess_name: 24000\n",
      "preprocess_name: 27000\n",
      "preprocess_description: 0\n",
      "preprocess_description: 3000\n",
      "preprocess_description: 6000\n",
      "preprocess_description: 9000\n",
      "preprocess_description: 12000\n",
      "preprocess_description: 15000\n",
      "preprocess_description: 18000\n",
      "preprocess_description: 21000\n",
      "preprocess_description: 24000\n",
      "preprocess_description: 27000\n",
      "preprocess_employer_name: 0\n",
      "preprocess_employer_name: 3000\n",
      "preprocess_employer_name: 6000\n",
      "preprocess_employer_name: 9000\n",
      "preprocess_employer_name: 12000\n",
      "preprocess_employer_name: 15000\n",
      "preprocess_employer_name: 18000\n",
      "preprocess_employer_name: 21000\n",
      "preprocess_employer_name: 24000\n",
      "preprocess_employer_name: 27000\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "\n",
    "num = 0\n",
    "\n",
    "nlp_ru = spacy.load(\"ru_core_news_lg\")\n",
    "nlp_en = spacy.load('en_core_web_lg')  \n",
    "\n",
    "def preprocess_name(text):\n",
    "    global num\n",
    "    string = text.replace(\"-\", \" \").replace(\"/\", \" \").replace(\".\", \"\")\n",
    "    doc = nlp_ru(string)       \n",
    "    ret = ' '.join([token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space])\n",
    "    if num % 3000 == 0:\n",
    "        print(\"preprocess_name:\", num)\n",
    "\n",
    "    num += 1\n",
    "    return ret\n",
    "\n",
    "def preprocess_description(text):\n",
    "    global num\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    strings = [string.text.strip() for string in soup.strings]\n",
    "    strings = [string for string in strings if string != '']\n",
    "    tokens = []\n",
    "    for doc in nlp_ru.pipe(strings):\n",
    "       proj_tok = ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha])\n",
    "       tokens.append(proj_tok)\n",
    "    ret = ' '.join(tokens)\n",
    "\n",
    "    if ret.strip() == '':\n",
    "        for doc in nlp_en.pipe(strings):\n",
    "           proj_tok = ' '.join([token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha])\n",
    "           tokens.append(proj_tok)\n",
    "        ret = ' '.join(tokens)\n",
    "\n",
    "    if num % 3000 == 0:\n",
    "        print(\"preprocess_description:\", num)\n",
    "\n",
    "    num += 1\n",
    "    return ret\n",
    "\n",
    "def preprocess_employer_name(string):\n",
    "    global num\n",
    "\n",
    "    doc = nlp_ru(string)       \n",
    "    ret = ' '.join([token.lemma_.lower() for token in doc if not token.is_punct])\n",
    "\n",
    "    if num % 3000 == 0:\n",
    "        print(\"preprocess_employer_name:\", num)\n",
    "\n",
    "    num += 1\n",
    "    \n",
    "    return ret\n",
    "    \n",
    "# X_test = X_test[:105]\n",
    "X_test['name'] = X_test['name'].apply(preprocess_name)\n",
    "num = 0\n",
    "X_test['description'] = X_test['description'].apply(preprocess_description)\n",
    "num = 0\n",
    "X_test['employer_name'] = X_test['employer_name'].apply(preprocess_employer_name)\n",
    "num = 0\n",
    "\n",
    "# train_data = train_data[:105]\n",
    "train_data['name'] = train_data['name'].apply(preprocess_name)\n",
    "num = 0\n",
    "train_data['description'] = train_data['description'].apply(preprocess_description)\n",
    "num = 0\n",
    "train_data['employer_name'] = train_data['employer_name'].apply(preprocess_employer_name)\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5294617-8980-4229-8982-137d42162c94",
   "metadata": {},
   "source": [
    "# Process binary and number columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "911bbad6-1b55-4072-8d8a-f409700ceae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(column):\n",
    "    col = column.fillna(column.mode().iloc[0])\n",
    "    return col.apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "binary_columns = ['has_test', 'response_letter_required', 'salary_gross']\n",
    "for column in binary_columns:\n",
    "    train_data[column] = convert_to_binary(train_data[column])\n",
    "    X_test[column] = convert_to_binary(X_test[column])\n",
    "\n",
    "median_salary_from = train_data['salary_from'].median()\n",
    "train_data['salary_from'].fillna(median_salary_from, inplace=True)\n",
    "\n",
    "median_salary_from = X_test['salary_from'].median()\n",
    "X_test['salary_from'].fillna(median_salary_from, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "87393704-7e44-4502-be78-28729eec6a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nans in train\n",
      " name                        0\n",
      "has_test                    0\n",
      "response_letter_required    0\n",
      "salary_from                 0\n",
      "salary_gross                0\n",
      "employer_name               0\n",
      "description                 0\n",
      "area_id                     0\n",
      "salary_to                   0\n",
      "dtype: int64\n",
      "\n",
      "Nans in train\n",
      " name                        0\n",
      "has_test                    0\n",
      "response_letter_required    0\n",
      "salary_from                 0\n",
      "salary_gross                0\n",
      "employer_name               0\n",
      "description                 0\n",
      "area_id                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "has_nan = train_data.isna().sum()\n",
    "print(\"Nans in train\\n\", has_nan)\n",
    "\n",
    "has_nan = X_test.isna().sum()\n",
    "print(\"\\nNans in train\\n\", has_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71010907-5d09-4d79-a7a2-fb8c173e2c0d",
   "metadata": {},
   "source": [
    "# Turn text columns into features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c03631e-813c-46ae-9670-ffe8111ad3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6486/1448953081.py:27: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  train_data = train_data.groupby(level=0, axis=1).max()\n",
      "/tmp/ipykernel_6486/1448953081.py:28: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
      "  X_test = X_test.groupby(level=0, axis=1).max()\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=2500, stop_words='english')\n",
    "\n",
    "text_columns = ['name', 'employer_name', 'description']\n",
    "\n",
    "for column in text_columns:\n",
    "    tfidf_vectorizer.fit(train_data[column])\n",
    "    \n",
    "# Replace the text column with its TF-IDF representation\n",
    "\n",
    "for column in text_columns:\n",
    "    train_data_tfidf = tfidf_vectorizer.transform(train_data[column])\n",
    "    train_data_tfidf_df = pd.DataFrame(train_data_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    train_data = pd.concat([train_data.drop(column, axis=1), train_data_tfidf_df], axis=1)\n",
    "\n",
    "for column in text_columns:\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test[column])\n",
    "\n",
    "    X_test_tfidf_df = pd.DataFrame(X_test_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "    X_test = pd.concat([X_test.drop(column, axis=1), X_test_tfidf_df], axis=1)\n",
    "\n",
    "# de-duplicate columns\n",
    "train_data = train_data.groupby(level=0, axis=1).max()\n",
    "X_test = X_test.groupby(level=0, axis=1).max()\n",
    "\n",
    "train_data.to_csv('processed_train_data.csv', index=False)\n",
    "X_test.to_csv('processed_X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8c8edb-c271-40bf-801e-b39189a86b10",
   "metadata": {},
   "source": [
    "# Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e63ddda-3e34-423c-8af9-adfa8ed4985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "train_data = pd.read_csv('processed_train_data.csv')\n",
    "\n",
    "X = train_data.drop(columns=['id', 'salary_to'], axis=1)\n",
    "y = train_data['salary_to']\n",
    "\n",
    "Xtrain, Xval, ytrain, yval = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd76e6-6f02-4acd-9380-3836c19861c2",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6229009-d31e-4066-8c15-03ffa40dc986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:101762.70131\tvalidation_1-rmse:87409.63984\n",
      "[100]\tvalidation_0-rmse:15619.19567\tvalidation_1-rmse:15260.74576\n",
      "[200]\tvalidation_0-rmse:10529.45478\tvalidation_1-rmse:10609.54099\n",
      "[300]\tvalidation_0-rmse:8599.86000\tvalidation_1-rmse:8785.30411\n",
      "[400]\tvalidation_0-rmse:7352.06860\tvalidation_1-rmse:7456.62524\n",
      "[500]\tvalidation_0-rmse:6390.51848\tvalidation_1-rmse:6477.59401\n",
      "[600]\tvalidation_0-rmse:5629.89710\tvalidation_1-rmse:5675.12337\n",
      "[700]\tvalidation_0-rmse:5037.49919\tvalidation_1-rmse:5005.15738\n",
      "[800]\tvalidation_0-rmse:4578.63575\tvalidation_1-rmse:4510.11208\n",
      "[900]\tvalidation_0-rmse:4168.31762\tvalidation_1-rmse:4116.13595\n",
      "[1000]\tvalidation_0-rmse:3829.65038\tvalidation_1-rmse:3720.25650\n",
      "[1100]\tvalidation_0-rmse:3530.22942\tvalidation_1-rmse:3412.46360\n",
      "[1200]\tvalidation_0-rmse:3271.16352\tvalidation_1-rmse:3168.75982\n",
      "[1300]\tvalidation_0-rmse:3052.27853\tvalidation_1-rmse:2930.24174\n",
      "[1400]\tvalidation_0-rmse:2850.82053\tvalidation_1-rmse:2723.20850\n",
      "[1500]\tvalidation_0-rmse:2703.97582\tvalidation_1-rmse:2593.58087\n",
      "[1600]\tvalidation_0-rmse:2552.53423\tvalidation_1-rmse:2414.43131\n",
      "[1700]\tvalidation_0-rmse:2433.88637\tvalidation_1-rmse:2286.12372\n",
      "[1800]\tvalidation_0-rmse:2314.47496\tvalidation_1-rmse:2147.69660\n",
      "[1900]\tvalidation_0-rmse:2210.71883\tvalidation_1-rmse:2043.61828\n",
      "[2000]\tvalidation_0-rmse:2120.49869\tvalidation_1-rmse:1953.59168\n",
      "[2100]\tvalidation_0-rmse:2047.61842\tvalidation_1-rmse:1868.72583\n",
      "[2200]\tvalidation_0-rmse:1975.22496\tvalidation_1-rmse:1790.82458\n",
      "[2300]\tvalidation_0-rmse:1914.65250\tvalidation_1-rmse:1723.84004\n",
      "[2400]\tvalidation_0-rmse:1867.57892\tvalidation_1-rmse:1680.62780\n",
      "[2500]\tvalidation_0-rmse:1822.13118\tvalidation_1-rmse:1634.82509\n",
      "[2600]\tvalidation_0-rmse:1788.46050\tvalidation_1-rmse:1600.70155\n",
      "[2700]\tvalidation_0-rmse:1754.15665\tvalidation_1-rmse:1567.45149\n",
      "[2800]\tvalidation_0-rmse:1725.50661\tvalidation_1-rmse:1537.50879\n",
      "[2900]\tvalidation_0-rmse:1700.54061\tvalidation_1-rmse:1513.90561\n",
      "[3000]\tvalidation_0-rmse:1678.55340\tvalidation_1-rmse:1493.60119\n",
      "[3100]\tvalidation_0-rmse:1658.58237\tvalidation_1-rmse:1472.66011\n",
      "[3200]\tvalidation_0-rmse:1641.43200\tvalidation_1-rmse:1455.90539\n",
      "[3300]\tvalidation_0-rmse:1625.74500\tvalidation_1-rmse:1442.43368\n",
      "[3400]\tvalidation_0-rmse:1613.02208\tvalidation_1-rmse:1431.51876\n",
      "[3500]\tvalidation_0-rmse:1601.37116\tvalidation_1-rmse:1420.11830\n",
      "[3600]\tvalidation_0-rmse:1590.15497\tvalidation_1-rmse:1409.97940\n",
      "[3700]\tvalidation_0-rmse:1581.04186\tvalidation_1-rmse:1399.80732\n",
      "[3800]\tvalidation_0-rmse:1572.80296\tvalidation_1-rmse:1390.47287\n",
      "[3900]\tvalidation_0-rmse:1565.26670\tvalidation_1-rmse:1384.33412\n",
      "[4000]\tvalidation_0-rmse:1558.80602\tvalidation_1-rmse:1377.19658\n",
      "[4100]\tvalidation_0-rmse:1552.64570\tvalidation_1-rmse:1371.11809\n",
      "[4200]\tvalidation_0-rmse:1548.05025\tvalidation_1-rmse:1367.58022\n",
      "[4300]\tvalidation_0-rmse:1543.17321\tvalidation_1-rmse:1362.24950\n",
      "[4400]\tvalidation_0-rmse:1539.17061\tvalidation_1-rmse:1357.63785\n",
      "[4500]\tvalidation_0-rmse:1535.51224\tvalidation_1-rmse:1355.71303\n",
      "[4600]\tvalidation_0-rmse:1532.38388\tvalidation_1-rmse:1352.93204\n",
      "[4700]\tvalidation_0-rmse:1529.40400\tvalidation_1-rmse:1349.90828\n",
      "[4800]\tvalidation_0-rmse:1527.02746\tvalidation_1-rmse:1346.17188\n",
      "[4900]\tvalidation_0-rmse:1525.05324\tvalidation_1-rmse:1344.30912\n",
      "[5000]\tvalidation_0-rmse:1522.98867\tvalidation_1-rmse:1342.01000\n",
      "[5100]\tvalidation_0-rmse:1521.14396\tvalidation_1-rmse:1339.79673\n",
      "[5200]\tvalidation_0-rmse:1519.47688\tvalidation_1-rmse:1338.43108\n",
      "[5300]\tvalidation_0-rmse:1517.92595\tvalidation_1-rmse:1336.70637\n",
      "[5400]\tvalidation_0-rmse:1516.43001\tvalidation_1-rmse:1335.18238\n",
      "[5500]\tvalidation_0-rmse:1515.17158\tvalidation_1-rmse:1332.59383\n",
      "[5600]\tvalidation_0-rmse:1514.02528\tvalidation_1-rmse:1331.49295\n",
      "[5700]\tvalidation_0-rmse:1513.02128\tvalidation_1-rmse:1330.96984\n",
      "[5800]\tvalidation_0-rmse:1512.06908\tvalidation_1-rmse:1330.15555\n",
      "[5900]\tvalidation_0-rmse:1511.29579\tvalidation_1-rmse:1329.04237\n",
      "[5999]\tvalidation_0-rmse:1510.63145\tvalidation_1-rmse:1328.42268\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBRegressor(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=&#x27;rmse&#x27;, feature_types=None, gamma=0.0,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=18, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=6000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBRegressor</label><div class=\"sk-toggleable__content\"><pre>XGBRegressor(base_score=None, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric=&#x27;rmse&#x27;, feature_types=None, gamma=0.0,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=18, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=6000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBRegressor(base_score=None, booster='gbtree', callbacks=None,\n",
       "             colsample_bylevel=0.5, colsample_bynode=None, colsample_bytree=0.7,\n",
       "             device=None, early_stopping_rounds=None, enable_categorical=False,\n",
       "             eval_metric='rmse', feature_types=None, gamma=0.0,\n",
       "             grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=18, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             multi_strategy=None, n_estimators=6000, n_jobs=-1,\n",
       "             num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "model = XGBRegressor(\n",
    "    booster = 'gbtree',\n",
    "    colsample_bylevel = 0.5,\n",
    "    colsample_bytree = 0.7,\n",
    "    gamma = 0.0,\n",
    "    learning_rate = 0.05,\n",
    "    max_depth = 18,\n",
    "    n_estimators = 6000,\n",
    "    n_jobs = -1,\n",
    "    seed = 42,\n",
    "    random_state=42,\n",
    "    eval_metric = 'rmse')\n",
    "\n",
    "model.fit(\n",
    "          X, y, \n",
    "          # Xtrain, ytrain, \n",
    "          verbose = 100,\n",
    "          eval_set = [(Xtrain, ytrain), (Xval, yval)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4f369-1445-4bf4-b899-b2b55aadc658",
   "metadata": {},
   "source": [
    "# Evaluate prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d54505-c760-4a3c-ac33-9d49491972ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(Xval)\n",
    "\n",
    "def smape(actual, predicted):\n",
    "    denominator = (abs(actual) + abs(predicted)) / 2.0\n",
    "    diff = abs(actual - predicted) / denominator\n",
    "    diff[denominator == 0] = 0.0\n",
    "    return 100 * diff.sum() / len(actual)\n",
    "\n",
    "smape_score = smape(yval, y_pred)\n",
    "print(f'SMAPE: {smape_score:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448ba286-d576-4fb5-b398-97dfea7a34c4",
   "metadata": {},
   "source": [
    "# Get results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98af6578-645f-4479-9885-6e712735b0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_test = pd.read_csv('processed_X_test.csv')\n",
    "test_predictions = model.predict(X_test.drop(columns=['id']))\n",
    "\n",
    "submission_df = pd.DataFrame({'id': X_test['id'].astype(int), 'salary_to': test_predictions.round(2)})\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
